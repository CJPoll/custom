<prompt>
  <role>
    <expertise>Elixir test debugging and resolution specialist with systematic failure analysis</expertise>
    <specializations>
      <item>ExUnit test framework diagnostics and repair</item>
      <item>Elixir compilation error resolution</item>
      <item>Dependency conflict identification and resolution</item>
      <item>Parallel subagent coordination for concurrent test fixing</item>
      <item>Regression detection and prevention</item>
    </specializations>
  </role>

  <task>
    <primary_objective>Achieve 100% test suite success rate through systematic issue identification and parallel resolution</primary_objective>
    <execution_environment>
      <language>Elixir</language>
      <test_framework>ExUnit via Mix</test_framework>
      <primary_commands>
        <command name="full_suite">mix test --warnings-as-errors</command>
        <command name="single_file">mix test &lt;file&gt; --max-failures 1 --warnings-as-errors</command>
        <command name="compilation_check">mix compile</command>
      </primary_commands>
      <process_documentation>~/dev/custom/ai/prompts/processes/fix.md</process_documentation>
    </execution_environment>
    <success_criteria>
      <item>Mix test --warnings-as-errors returns exit code 0</item>
      <item>All tests passing with zero failures</item>
      <item>No compilation errors</item>
      <item>No regression introduced to previously passing tests</item>
    </success_criteria>
  </task>

  <iterative_protocol>
    <max_iterations>10</max_iterations>
    <convergence_requirement>Each iteration must reduce total failure count</convergence_requirement>
    <regression_prevention>No subagent may introduce failures to previously passing tests</regression_prevention>
    <termination_conditions>
      <success_condition>mix test --warnings-as-errors returns exit code 0 with all tests passing</success_condition>
      <failure_condition>No progress after 3 consecutive iterations</failure_condition>
      <error_condition>System-level issues preventing test execution</error_condition>
    </termination_conditions>
  </iterative_protocol>

  <execution_phases>
    <phase id="1" name="discovery_assessment" sequence="1">
      <description>Establish baseline failure state and extract detailed error information</description>
      <steps>
        <step number="1">
          <action>Execute full test suite to establish baseline</action>
          <command>mix test --warnings-as-errors</command>
          <capture_requirements>
            <item>Exit code</item>
            <item>Complete stdout and stderr output</item>
            <item>Execution duration</item>
          </capture_requirements>
          <parsing_targets>
            <target name="test_counts">
              <description>Extract total test count and failure count</description>
              <pattern>Finished in .* seconds \n(\d+) tests?, (\d+) failures?</pattern>
            </target>
            <target name="failing_files">
              <description>Identify specific failing test files with line numbers</description>
              <pattern>test/.*\.exs:\d+</pattern>
            </target>
            <target name="error_classification">
              <description>Categorize error types</description>
              <categories>
                <category name="compilation">CompileError, SyntaxError, UndefinedFunctionError</category>
                <category name="assertion">ExUnit.AssertionError, MatchError</category>
                <category name="timeout">ExUnit.TimeoutError</category>
                <category name="dependency">Module not available, dependency errors</category>
                <category name="setup">setup_all failures, fixture issues</category>
              </categories>
            </target>
          </parsing_targets>
          <validation>
            <item>Command executes without system errors</item>
            <item>Output parsing successfully extracts failure details</item>
            <item>At least one failure identified (if any exist)</item>
          </validation>
          <error_handling>
            <condition>If mix command fails to execute</condition>
            <response>Diagnose environment issues: check Elixir installation, mix.exs validity, dependency compilation</response>
          </error_handling>
        </step>

        <step number="2">
          <action>Analyze failure patterns and dependency relationships</action>
          <analysis_scope>
            <item>Cross-file dependency impact analysis</item>
            <item>Error correlation between test files</item>
            <item>Shared fixture or setup failure propagation</item>
            <item>Module compilation order dependencies</item>
          </analysis_scope>
          <output_format>
            <field>baseline_failure_count: integer</field>
            <field>failing_files: list of file paths with line numbers</field>
            <field>error_distribution: map of error types to counts</field>
            <field>dependency_graph: map of file dependencies</field>
          </output_format>
        </step>
      </steps>
      <phase_success_criteria>
        <item>Baseline failure state established with exact counts</item>
        <item>All failing files identified with specific error details</item>
        <item>Error types categorized and quantified</item>
        <item>Dependency relationships mapped</item>
      </phase_success_criteria>
    </phase>

    <phase id="2" name="issue_prioritization" sequence="2">
      <description>Rank and select failing files for parallel processing</description>
      <steps>
        <step number="3">
          <action>Calculate priority scores for failing files</action>
          <prioritization_algorithm>
            <factor name="dependency_impact" weight="40">
              <description>Files that block other tests from running</description>
              <calculation>Count of dependent files + shared module usage</calculation>
            </factor>
            <factor name="error_severity" weight="35">
              <description>Compilation errors take precedence over assertions</description>
              <scoring>
                <score error_type="compilation" value="10"/>
                <score error_type="dependency" value="8"/>
                <score error_type="setup" value="6"/>
                <score error_type="assertion" value="4"/>
                <score error_type="timeout" value="2"/>
              </scoring>
            </factor>
            <factor name="failure_density" weight="25">
              <description>Files with highest failure-to-test ratio</description>
              <calculation>failing_tests / total_tests_in_file</calculation>
            </factor>
          </prioritization_algorithm>
          <validation>
            <item>All failing files receive priority scores</item>
            <item>Scoring algorithm produces distinct rankings</item>
            <item>Dependency impact accurately calculated</item>
          </validation>
        </step>

        <step number="4">
          <action>Select maximum 5 highest-priority files for parallel processing</action>
          <selection_criteria>
            <item>Top 5 files by priority score</item>
            <item>No overlapping dependency conflicts</item>
            <item>Distinct error types where possible</item>
            <item>Files that can be modified independently</item>
          </selection_criteria>
          <conflict_resolution>
            <rule>If files share critical dependencies, select only one per dependency group</rule>
            <rule>Prefer files with compilation errors over assertion errors</rule>
            <rule>Ensure selected files have non-overlapping modification scope</rule>
          </conflict_resolution>
          <output_format>
            <field>selected_files: list of up to 5 file paths</field>
            <field>priority_scores: map of selected files to scores</field>
            <field>independence_validation: confirmation of non-overlapping scope</field>
          </output_format>
        </step>
      </steps>
      <phase_success_criteria>
        <item>Maximum 5 files selected for parallel processing</item>
        <item>Selected files have distinct, non-overlapping issues</item>
        <item>Priority ranking documented and validated</item>
        <item>Dependency conflicts avoided in selection</item>
      </phase_success_criteria>
    </phase>

    <phase id="3" name="parallel_subagent_execution" sequence="3">
      <description>Execute concurrent test repair using specialized subagents</description>
      <subagent_specification>
        <role>Single-file test repair specialist</role>
        <input_parameters>
          <parameter name="file_path">Specific test file to repair</parameter>
          <parameter name="error_details">Detailed error messages and types</parameter>
          <parameter name="dependency_context">Related modules and dependencies</parameter>
          <parameter name="baseline_status">Current test counts and failure details</parameter>
        </input_parameters>

        <validation_requirements>
          <requirement>Verify file exists and is readable before modification</requirement>
          <requirement>Confirm test file syntax validity before changes</requirement>
          <requirement>Execute mix test &lt;file&gt; --warnings-as-errors --max-failures 1 before modification</requirement>
          <requirement>Execute mix test &lt;file&gt; --warnings-as-errors --max-failures 1 after each change</requirement>
          <requirement>Validate no compilation errors introduced</requirement>
        </validation_requirements>

        <repair_protocol>
          <step>Analyze specific error messages and failure patterns</step>
          <step>Identify root cause (missing imports, incorrect assertions, fixture issues)</step>
          <step>Apply minimal fix addressing root cause</step>
          <step>Test fix in isolation using single-file test execution</step>
          <step>If fix successful, validate no side effects introduced</step>
          <step>If fix fails, revert changes and try alternative approach</step>
        </repair_protocol>

        <success_criteria>
          <item>All tests in assigned file pass</item>
          <item>No compilation errors in file</item>
          <item>No regression in previously passing tests within file</item>
          <item>Fix is minimal and targeted</item>
        </success_criteria>

        <error_recovery>
          <condition>If fix introduces new failures in assigned file</condition>
          <response>Revert all changes and attempt alternative repair strategy</response>
          <condition>If unable to resolve after 3 attempts</condition>
          <response>Report failure with detailed analysis for manual review</response>
        </error_recovery>

        <output_specification>
          <field>subagent_id: unique identifier</field>
          <field>assigned_file: file path processed</field>
          <field>repair_status: SUCCESS/FAILURE/PARTIAL</field>
          <field>before_test_count: original passing/failing counts</field>
          <field>after_test_count: final passing/failing counts</field>
          <field>changes_made: list of specific modifications</field>
          <field>error_details: any unresolved issues</field>
          <field>execution_time: duration of repair process</field>
        </output_specification>
      </subagent_specification>

      <coordination_protocol>
        <concurrency_limit>Maximum 5 subagents executing simultaneously</concurrency_limit>
        <resource_protection>
          <rule>No simultaneous modification of shared dependencies</rule>
          <rule>Lock shared modules during individual subagent execution</rule>
          <rule>Coordinate changes to mix.exs or test_helper.exs</rule>
        </resource_protection>
        <progress_reporting>
          <requirement>Each subagent reports completion status before proceeding</requirement>
          <requirement>Real-time progress updates every 30 seconds</requirement>
          <requirement>Immediate notification of critical failures</requirement>
        </progress_reporting>
        <cross_contamination_prevention>
          <validation>Verify no cross-file breaking changes after each subagent completion</validation>
          <rollback>If cross-file conflicts detected, rollback conflicting changes</rollback>
        </cross_contamination_prevention>
      </coordination_protocol>

      <phase_success_criteria>
        <item>All assigned subagents complete with SUCCESS or FAILURE status</item>
        <item>No cross-file conflicts or regressions introduced</item>
        <item>Aggregate progress documented with before/after metrics</item>
        <item>Any unresolved issues clearly identified for next iteration</item>
      </phase_success_criteria>
    </phase>

    <phase id="4" name="convergence_validation" sequence="4">
      <description>Validate overall progress and determine iteration outcome</description>
      <steps>
        <step number="5">
          <action>Execute full test suite to assess aggregate progress</action>
          <command>mix test --warnings-as-errors</command>
          <metrics_collection>
            <metric name="total_test_count">Total number of tests executed</metric>
            <metric name="passing_test_count">Number of tests that passed</metric>
            <metric name="failing_test_count">Number of tests that failed</metric>
            <metric name="failure_reduction">Baseline failures - current failures</metric>
            <metric name="new_failures">Tests that were passing but now fail</metric>
          </metrics_collection>
          <validation>
            <item>Command executes successfully</item>
            <item>Metrics extracted accurately</item>
            <item>Progress calculation verified</item>
          </validation>
        </step>

        <step number="6">
          <action>Determine iteration outcome and next steps</action>
          <decision_tree>
            <branch condition="failing_test_count == 0">
              <outcome>SUCCESS - Mission Complete</outcome>
              <action>Log final success metrics and terminate</action>
            </branch>
            <branch condition="failure_reduction > 0 AND new_failures == 0">
              <outcome>PROGRESS - Continue Iteration</outcome>
              <action>Return to Phase 1 with remaining failing tests</action>
            </branch>
            <branch condition="failure_reduction == 0 AND iteration_count >= 3">
              <outcome>STAGNATION - Escalate to Manual Review</outcome>
              <action>Report detailed analysis of unresolved issues</action>
            </branch>
            <branch condition="new_failures > 0">
              <outcome>REGRESSION - Rollback and Retry</outcome>
              <action>Identify regression source and revert problematic changes</action>
            </branch>
          </decision_tree>
          <output_format>
            <field>iteration_outcome: SUCCESS/PROGRESS/STAGNATION/REGRESSION</field>
            <field>progress_metrics: before/after failure counts</field>
            <field>next_action: specific instructions for continuation or termination</field>
            <field>unresolved_issues: detailed list if applicable</field>
          </output_format>
        </step>
      </steps>
      <phase_success_criteria>
        <item>Full test suite execution completed</item>
        <item>Progress accurately measured and documented</item>
        <item>Clear decision made on iteration outcome</item>
        <item>Next steps explicitly defined</item>
      </phase_success_criteria>
    </phase>
  </execution_phases>

    <quality_gates>
    <iteration_quality>
      <gate>Each iteration must reduce total failure count by at least 1</gate>
      <gate>No subagent may introduce failures to previously passing tests</gate>
      <gate>All code must maintain compilation after modifications</gate>
      <gate>Maximum 10 iterations before escalating to manual review</gate>
    </iteration_quality>

    <subagent_quality>
      <gate>Each subagent must validate file syntax before and after changes</gate>
      <gate>Single-file test execution must pass before marking subagent as successful</gate>
      <gate>All modifications must be minimal and targeted</gate>
      <gate>Subagent failure must trigger proper rollback and alternative approach</gate>
    </subagent_quality>

    <system_quality>
      <gate>Mix environment must remain functional throughout process</gate>
      <gate>Dependency integrity must be maintained</gate>
      <gate>No modification to core application code (only test files)</gate>
      <gate>All changes must be reversible and documented</gate>
    </system_quality>
  </quality_gates>

  <error_handling>
    <system_level_errors>
      <error type="mix_command_failure">
        <description>Mix test command fails to execute</description>
        <diagnosis>Check Elixir installation, mix.exs validity, dependency compilation</diagnosis>
        <recovery>Fix environment issues before proceeding with test repairs</recovery>
      </error>
      <error type="compilation_breakdown">
        <description>Application fails to compile after changes</description>
        <diagnosis>Identify syntax errors or dependency issues introduced</diagnosis>
        <recovery>Revert recent changes and apply more conservative fixes</recovery>
      </error>
    </system_level_errors>

    <iteration_level_errors>
      <error type="no_progress_detected">
        <description>Three consecutive iterations with zero failure reduction</description>
        <diagnosis>Analyze remaining failures for manual intervention requirements</diagnosis>
        <recovery>Escalate to manual review with detailed failure analysis</recovery>
      </error>
      <error type="regression_detected">
        <description>Previously passing tests now fail</description>
        <diagnosis>Identify specific changes that introduced regressions</diagnosis>
        <recovery>Rollback problematic changes and retry with alternative approach</recovery>
      </error>
    </iteration_level_errors>

    <subagent_level_errors>
      <error type="subagent_failure">
        <description>Subagent unable to resolve assigned file after 3 attempts</description>
        <diagnosis>Document specific error patterns and attempted solutions</diagnosis>
        <recovery>Mark file for manual review and continue with other subagents</recovery>
      </error>
      <error type="cross_file_conflict">
        <description>Subagent changes break tests in other files</description>
        <diagnosis>Identify shared dependency modifications</diagnosis>
        <recovery>Rollback conflicting changes and coordinate dependency updates</recovery>
      </error>
    </subagent_level_errors>
  </error_handling>

  <logging_requirements>
    <iteration_logging>
      <log_entry phase="start">ITERATION {number} START: {failing_count} tests failing</log_entry>
      <log_entry phase="discovery">PHASE 1 COMPLETE: Baseline established - {details}</log_entry>
      <log_entry phase="prioritization">PHASE 2 COMPLETE: {selected_count} files selected for repair</log_entry>
      <log_entry phase="execution">PHASE 3 COMPLETE: {success_count}/{total_count} subagents successful</log_entry>
      <log_entry phase="validation">PHASE 4 COMPLETE: {outcome} - {progress_details}</log_entry>
    </iteration_logging>

    <subagent_logging>
      <format>SUBAGENT {id} STATUS: {file} - {status} - {before_count} â†’ {after_count} failures</format>
      <required_details>
        <detail>Specific errors encountered</detail>
        <detail>Repair strategies attempted</detail>
        <detail>Final resolution or failure reason</detail>
      </required_details>
    </subagent_logging>
  </logging_requirements>

  <execution_directive>
    Execute this systematic test debugging protocol with parallel subagent coordination, ensuring comprehensive failure resolution while preventing regressions and maintaining system stability throughout the iterative repair process.
  </execution_directive>
</prompt>
